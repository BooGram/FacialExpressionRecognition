{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPe9FXuvH8utLSkjO3KdZro",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BooGram/FacialExpressionRecognition/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "o2WK-H4M2aD9",
        "outputId": "4e9016c0-3fb2-4036-e355-fb4032907dcf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-abae790a8b75>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "class_names = [\"Angry\", \"Happy\", \"Sad\", \"Surprise\"]\n",
        "\n",
        "def detect_emotion(frame):\n",
        "    model = tf.keras.models.load_model(\"model.h5\")\n",
        "\n",
        "    emotion = list(model.predict(tf.expand_dims(frame, axis=0)))\n",
        "    num = max(emotion[0])\n",
        "    idx = list(emotion[0]).index(num)\n",
        "\n",
        "    return idx, num\n",
        "\n",
        "\n",
        "def preprocess(frame):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame = cv2.resize(frame, (48, 48))\n",
        "    frame = frame / 255.\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "def detect_face(frame):\n",
        "    cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    faces = cascade.detectMultiScale(gray, 1.1, 4)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "\n",
        "    return gray, frame, faces\n",
        "\n",
        "\n",
        "video = cv2.VideoCapture(0)\n",
        "\n",
        "while (True):\n",
        "\n",
        "    _, frame = video.read()\n",
        "\n",
        "    if _:\n",
        "        gray, frame, coordinates = detect_face(frame)\n",
        "\n",
        "        if coordinates is not None:\n",
        "            gray = gray[coordinates[0][1]:coordinates[0][1] + coordinates[0][3], coordinates[0][0]:coordinates[0][0] + coordinates[0][2]]\n",
        "            process_img = preprocess(gray)\n",
        "            idx, conf = detect_emotion(process_img)\n",
        "\n",
        "            class_name = class_names[idx]\n",
        "\n",
        "            if conf > 0.3 and coordinates is not None:\n",
        "                cv2.putText(frame, class_name, (coordinates[0][0], coordinates[0][1]), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
        "                            (0, 255, 0), 2)\n",
        "\n",
        "            cv2.imshow(\"Window\", frame)\n",
        "            cv2.waitKey(0)"
      ]
    }
  ]
}